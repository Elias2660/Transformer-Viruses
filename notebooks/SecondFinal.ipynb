{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgIXlfAeyana"
      },
      "source": [
        "## Create the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK98N5Eom3Rt"
      },
      "outputs": [],
      "source": [
        "# define a vocabulary, this should be a mapping from all of the possible characters\n",
        "# that can appear in our sequence\n",
        "# we can either do this manually by defining a dictionary and writing {'A': 0, 'G': 1, 'T': 2, ...}\n",
        "# or you can use torchtext.vocab.build_vocab_from_iterator (this is probably easier)\n",
        "\n",
        "sample_sequence = list_of_genomic_file_contents[0]\n",
        "vocab = None # write code here\n",
        "print(\"Vocab is: {}\".format(vocab)) # sanity check that all of the possible values are reflected\n",
        "\n",
        "# now we builder our tokenizer\n",
        "tokenizer = torch.classes.torchtext.Tokenizer(vocab=vocab, split='character')\n",
        "sample_tokens = tokenizer(sample_sequence) # sanity check that it looks good"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install some necessary libraries"
      ],
      "metadata": {
        "id": "vxJENy6-rZ7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkLPZdZaycfq"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MD2z6Zijw_a"
      },
      "source": [
        "## Create our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OmkRYuRn6rk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Pytorch defines a nice dataset class that only requires we implement two functions:\n",
        "# 1. __len__\n",
        "# 2. __getitem__\n",
        "# Our dataset is just the set of all samples that we want to train our model on\n",
        "# __len__ should get us the total number of samples\n",
        "# __getitem__ takes in an integer and should give us the corresponding element in the dataset\n",
        "\n",
        "# You might be wondering why they have those weird underscores? That enables us to\n",
        "# call those functions on the class instances directly\n",
        "#\n",
        "# For instance if we define an instance of our Dataset class, then we can get the length of it\n",
        "# by invoking `len` on it directly\n",
        "# new_dataset = GenomicR0ValueDataset(some_sample_sequences, some_sample_r0_values, new_tokenizer)\n",
        "# len(new_dataset) # this will call GenomicR0ValueDataset.__len__\n",
        "\n",
        "class GenomicR0ValueDataset(Dataset):\n",
        "  def __init__(self, sequences, r0_values, tokenizer):\n",
        "    # here we should initialize some class variables using `self.` so that\n",
        "    # we can access them further along\n",
        "    # make sure all of our sequences are tokenized so that when we return them in __getitem__\n",
        "    # we don't have to do any post-processing on them during training\n",
        "\n",
        "  def __len__(self):\n",
        "    # this needs to return the number of elements in our dataset\n",
        "    # so this should be the total number of sequences\n",
        "    pass\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # get item should take in an index and return the corresponding genomic sequence\n",
        "    # AND its r0 value. we need to return both because every time we pass a sequence\n",
        "    # through our model, we need to compare its predicted r0 value to the correct r0\n",
        "    # value. so the format of this output should be returning two things like this:\n",
        "    # return _, _\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIY2CYd3kwP2"
      },
      "source": [
        "## Create our dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxqTVvWWkzwB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "\n",
        "# pytorch dataloaders: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "# dataloaders take in a pytorch.Dataset as an argument (like the one we defined above!)\n",
        "\n",
        "dataset = None # lets instantiate that GenomicR0ValueDataset we defined above\n",
        "\n",
        "# define some train test split\n",
        "train_test_ratio = 0.9\n",
        "train_size = int(train_test_ratio * len(dataset)) # how convenient that we can called `len` on our dataset!!\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# now use random_split from torch.utils.data to define our two datasets\n",
        "train_dataset, test_dataset = None, None\n",
        "\n",
        "# define our batch_size\n",
        "batch_size = 4 # batch size defines how many sequences our model will be processing at once\n",
        "# higher batch sizes mean training will be faster, but will make updates slightly less precisely (will talk about this in person)\n",
        "# another thing to keep in mind is the amount of MEMORY that we have! our batches can't get too big!\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giSyBftTwA6N"
      },
      "source": [
        "## Define our model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkA00-MPyUuJ"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRkJTYVoyIwT"
      },
      "outputs": [],
      "source": [
        "# positional encoding explanation\n",
        "# https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "# positional encoding is critical for the transformer model\n",
        "# d_model is the embedding dimension of the model\n",
        "# max_len here is our context window size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUcM42aowCOP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "\n",
        "class GenomeR0ValueModel(nn.Module):\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[seq_len, batch_size]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# here, we define the model class, it is based on a transformer architecture\n",
        "# and it takes in a ntoken, which is the size of the vocabulary (number of unique characters in our input)\n",
        "# d_model, which is the embedding size of the tokens\n",
        "# nhead is the number of heads in our self-attention setup\n",
        "# d_hid is the dimension of the hidden layer\n",
        "# n_layers is the number of hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcYFOPUmk0Wb"
      },
      "source": [
        "## Create our train and test loop and begin trainin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIHPyJhFk1to"
      },
      "outputs": [],
      "source": [
        "# initialize our model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = None\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm  # Progress bar (optional, but very helpful)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "  # move the\n",
        "  model.to(device)\n",
        "  best_val_loss = float(\"inf\")\n",
        "  for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for inputs, targets in pbar:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets.unsqueeze(1))  # Ensure target shape matches output\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        pbar.set_postfix({\"Train Loss\": loss.item()})\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Test Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for inputs, targets in val_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets.unsqueeze(1))\n",
        "        val_loss += loss.item()\n",
        "      val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "train_loader = DataLoader(...)  # Your training dataloader (we defined these above)\n",
        "val_loader = DataLoader(...)   # Your validation dataloader\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(learning_rate=0.1)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuH9xs8yk2Wj"
      },
      "source": [
        "## Begin training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AzqFvx5k6hK"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}